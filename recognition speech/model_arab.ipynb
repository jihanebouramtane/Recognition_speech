{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11901969,"sourceType":"datasetVersion","datasetId":7481763},{"sourceId":11901987,"sourceType":"datasetVersion","datasetId":7481779}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers datasets torchaudio jiwer --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T13:00:54.970393Z","iopub.execute_input":"2025-05-22T13:00:54.970702Z","iopub.status.idle":"2025-05-22T13:02:37.329730Z","shell.execute_reply.started":"2025-05-22T13:00:54.970681Z","shell.execute_reply":"2025-05-22T13:02:37.328568Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"\nfrom datasets import load_dataset, concatenate_datasets, DatasetDict\n\n# Charger directement splits train, validation et test (version publique Common Voice)\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"ar\", split={\n    \"train\": \"train[:8000]\",\n    \"validation\": \"validation[:1000]\",\n    \"test\": \"test[:1000]\"\n})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:41:55.527621Z","iopub.execute_input":"2025-05-21T12:41:55.527857Z","iopub.status.idle":"2025-05-21T12:43:28.689200Z","shell.execute_reply.started":"2025-05-21T12:41:55.527835Z","shell.execute_reply":"2025-05-21T12:43:28.688672Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/14.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44f594aa26f34099ad32873328db0dca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"common_voice_11_0.py:   0%|          | 0.00/8.13k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bfbd3b908344c21be8a135d4431b1cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"languages.py:   0%|          | 0.00/3.44k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3ca4dc69ac844fbbfe918a2c6ff4b69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"release_stats.py:   0%|          | 0.00/60.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"978b8a1b08fe44898429c82a406e7aa4"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for mozilla-foundation/common_voice_11_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_11_0.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  Y\n"},{"output_type":"display_data","data":{"text/plain":"n_shards.json:   0%|          | 0.00/12.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa81d3301b6146ec9b89a8ae4770cfa4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"audio/ar/train/ar_train_0.tar:   0%|          | 0.00/712M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a396ff05d895441aa7621d6fc8b6d72e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"audio/ar/dev/ar_dev_0.tar:   0%|          | 0.00/300M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b372b31976344dc842357a85fc83b76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"audio/ar/test/ar_test_0.tar:   0%|          | 0.00/312M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"532ecec387f946d0a86441f75fafdb87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"audio/ar/other/ar_other_0.tar:   0%|          | 0.00/978M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"862d8ca3ab8442a8be5f2712450c9735"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"audio/ar/invalidated/ar_invalidated_0.ta(…):   0%|          | 0.00/449M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9cbf9034ffa4c96bbbff23f8044926e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"transcript/ar/train.tsv:   0%|          | 0.00/6.90M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41cd0551fab64e62b91a13d9cdf463b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"transcript/ar/dev.tsv:   0%|          | 0.00/2.52M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a67a25f20b7847379260213a205297df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"transcript/ar/test.tsv:   0%|          | 0.00/2.41M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"467282b327e54b79ab161f9301456fdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"transcript/ar/other.tsv:   0%|          | 0.00/8.44M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fefb82d4f7614b8aae9f0d8192656447"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"transcript/ar/invalidated.tsv:   0%|          | 0.00/3.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85d777c9bd3b40e29675b2d08ed1c4c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f1b9a469a524d60abb98345c5719c6b"}},"metadata":{}},{"name":"stderr","text":"\nReading metadata...: 0it [00:00, ?it/s]\u001b[A\nReading metadata...: 13649it [00:00, 136479.72it/s]\u001b[A\nReading metadata...: 28043it [00:00, 129996.11it/s]\u001b[A\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7110a151a2524aefa21fcd4f9050d69e"}},"metadata":{}},{"name":"stderr","text":"\nReading metadata...: 10438it [00:00, 148768.35it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84d9b29900de460ca2aaa430c36849ce"}},"metadata":{}},{"name":"stderr","text":"\nReading metadata...: 10440it [00:00, 172053.05it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating other split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afccb6bfbc4e462c923de51390fe2ffb"}},"metadata":{}},{"name":"stderr","text":"\nReading metadata...: 0it [00:00, ?it/s]\u001b[A\nReading metadata...: 17192it [00:00, 171902.95it/s]\u001b[A\nReading metadata...: 35514it [00:00, 157256.18it/s]\u001b[A\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating invalidated split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f418ac4e9514149b6f19d2395406591"}},"metadata":{}},{"name":"stderr","text":"\nReading metadata...: 14959it [00:00, 169491.21it/s]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import re\nfrom datasets import load_dataset, concatenate_datasets, DatasetDict\n\ndef normalize_arabic(text):\n    text = re.sub(r\"[^\\u0621-\\u064A ]+\", \"\", text)  # Supprime tout sauf lettres arabes\n    return text.strip()\n\ndataset = dataset.map(lambda x: {\"sentence\": normalize_arabic(x[\"sentence\"])})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:43:28.689933Z","iopub.execute_input":"2025-05-21T12:43:28.690255Z","iopub.status.idle":"2025-05-21T12:43:29.916364Z","shell.execute_reply.started":"2025-05-21T12:43:28.690237Z","shell.execute_reply":"2025-05-21T12:43:29.915779Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16fdb63e7da54922af63815448e45f14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ef43bc41c0449db9645523ec49460d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a19a28b4cf1f4adcb95402d326130ca6"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\nmodel_id = \"jonatasgrosman/wav2vec2-large-xlsr-53-arabic\"\nprocessor = Wav2Vec2Processor.from_pretrained(model_id)\nmodel = Wav2Vec2ForCTC.from_pretrained(model_id)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:43:29.917725Z","iopub.execute_input":"2025-05-21T12:43:29.917916Z","iopub.status.idle":"2025-05-21T12:44:02.543162Z","shell.execute_reply.started":"2025-05-21T12:43:29.917901Z","shell.execute_reply":"2025-05-21T12:44:02.542627Z"}},"outputs":[{"name":"stderr","text":"2025-05-21 12:43:42.081075: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747831422.264432      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747831422.315245      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/158 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eaf16438da0d45e7a22b0138282d89a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8570393e25cd4b16ab0f0e8cfa00db19"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/507 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df6d10900c76435a9ef1e450cb4ce576"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee66cd48800e419fa9a74a37a33a8886"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.26G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0dd9cf9eb924743922c498a2fdc3e3c"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"def prepare_batch(batch):\n    audio = batch[\"audio\"]\n    inputs = processor(audio[\"array\"], sampling_rate=16000, return_tensors=\"pt\", padding=True)\n    with processor.as_target_processor():\n        labels = processor(batch[\"sentence\"]).input_ids\n    batch[\"input_values\"] = inputs.input_values[0]\n    batch[\"labels\"] = labels\n    return batch\n\ndataset = dataset.map(prepare_batch, remove_columns=dataset[\"train\"].column_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:44:02.543858Z","iopub.execute_input":"2025-05-21T12:44:02.544420Z","iopub.status.idle":"2025-05-21T12:46:50.486445Z","shell.execute_reply.started":"2025-05-21T12:44:02.544383Z","shell.execute_reply":"2025-05-21T12:46:50.485477Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8db53a2bc6b45649d836e92a27ca339"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.26G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c80cd1ac4dda4e579f5faae45ed505a7"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f8dd6a0c5f64583b69ed3541f31ec85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4715a8e06a704e2f850266ac444f4140"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import List, Dict, Union\nimport torch\n\n@dataclass\nclass DataCollatorCTCWithPadding:\n    processor: any\n    padding: Union[bool, str] = True\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n\n        batch = self.processor.pad(input_features, padding=self.padding, return_tensors=\"pt\")\n\n        with self.processor.as_target_processor():\n            labels_batch = self.processor.pad(label_features, padding=self.padding, return_tensors=\"pt\")\n\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n        batch[\"labels\"] = labels\n\n        return batch\n\n# Ensuite :\ndata_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:47:22.682047Z","iopub.execute_input":"2025-05-21T12:47:22.682768Z","iopub.status.idle":"2025-05-21T12:47:22.692935Z","shell.execute_reply.started":"2025-05-21T12:47:22.682728Z","shell.execute_reply":"2025-05-21T12:47:22.692069Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, TrainingArguments, Trainer\ntraining_args = TrainingArguments(\n    output_dir=\"./wav2vec2-arabic-testA\",\n    per_device_train_batch_size=2,\n    save_strategy=\"epoch\",\n    learning_rate=3e-4,\n    num_train_epochs=6,\n    logging_steps=50,\n    fp16=True,\n    report_to=\"none\",  # Évite les blocages W&B sur Kaggle\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"validation\"],\n    tokenizer=processor.feature_extractor,\n    data_collator=data_collator,\n)\n\n\n``\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T15:00:40.586665Z","iopub.execute_input":"2025-05-21T15:00:40.586949Z","iopub.status.idle":"2025-05-21T15:00:40.627761Z","shell.execute_reply.started":"2025-05-21T15:00:40.586930Z","shell.execute_reply":"2025-05-21T15:00:40.626947Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/2629071980.py:13: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"\ntrainer.train(resume_from_checkpoint=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T17:55:51.101708Z","iopub.execute_input":"2025-05-21T17:55:51.102160Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='17455' max='24000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [17455/24000 3:45:38 < 2:36:13, 0.70 it/s, Epoch 4.36/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>8050</td>\n      <td>0.840900</td>\n    </tr>\n    <tr>\n      <td>8100</td>\n      <td>0.904100</td>\n    </tr>\n    <tr>\n      <td>8150</td>\n      <td>0.905900</td>\n    </tr>\n    <tr>\n      <td>8200</td>\n      <td>0.856700</td>\n    </tr>\n    <tr>\n      <td>8250</td>\n      <td>0.883200</td>\n    </tr>\n    <tr>\n      <td>8300</td>\n      <td>0.913700</td>\n    </tr>\n    <tr>\n      <td>8350</td>\n      <td>0.946500</td>\n    </tr>\n    <tr>\n      <td>8400</td>\n      <td>0.838500</td>\n    </tr>\n    <tr>\n      <td>8450</td>\n      <td>0.860500</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.891000</td>\n    </tr>\n    <tr>\n      <td>8550</td>\n      <td>0.850300</td>\n    </tr>\n    <tr>\n      <td>8600</td>\n      <td>0.903500</td>\n    </tr>\n    <tr>\n      <td>8650</td>\n      <td>0.840700</td>\n    </tr>\n    <tr>\n      <td>8700</td>\n      <td>0.869500</td>\n    </tr>\n    <tr>\n      <td>8750</td>\n      <td>0.807100</td>\n    </tr>\n    <tr>\n      <td>8800</td>\n      <td>0.874700</td>\n    </tr>\n    <tr>\n      <td>8850</td>\n      <td>0.930700</td>\n    </tr>\n    <tr>\n      <td>8900</td>\n      <td>0.846200</td>\n    </tr>\n    <tr>\n      <td>8950</td>\n      <td>0.888500</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.854300</td>\n    </tr>\n    <tr>\n      <td>9050</td>\n      <td>0.824700</td>\n    </tr>\n    <tr>\n      <td>9100</td>\n      <td>0.883000</td>\n    </tr>\n    <tr>\n      <td>9150</td>\n      <td>0.782000</td>\n    </tr>\n    <tr>\n      <td>9200</td>\n      <td>0.815200</td>\n    </tr>\n    <tr>\n      <td>9250</td>\n      <td>0.957100</td>\n    </tr>\n    <tr>\n      <td>9300</td>\n      <td>0.860400</td>\n    </tr>\n    <tr>\n      <td>9350</td>\n      <td>0.843400</td>\n    </tr>\n    <tr>\n      <td>9400</td>\n      <td>0.853100</td>\n    </tr>\n    <tr>\n      <td>9450</td>\n      <td>0.878500</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.895800</td>\n    </tr>\n    <tr>\n      <td>9550</td>\n      <td>0.805200</td>\n    </tr>\n    <tr>\n      <td>9600</td>\n      <td>0.864300</td>\n    </tr>\n    <tr>\n      <td>9650</td>\n      <td>0.756700</td>\n    </tr>\n    <tr>\n      <td>9700</td>\n      <td>0.812100</td>\n    </tr>\n    <tr>\n      <td>9750</td>\n      <td>0.816400</td>\n    </tr>\n    <tr>\n      <td>9800</td>\n      <td>0.820800</td>\n    </tr>\n    <tr>\n      <td>9850</td>\n      <td>0.755000</td>\n    </tr>\n    <tr>\n      <td>9900</td>\n      <td>0.786900</td>\n    </tr>\n    <tr>\n      <td>9950</td>\n      <td>0.817400</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.869500</td>\n    </tr>\n    <tr>\n      <td>10050</td>\n      <td>0.825000</td>\n    </tr>\n    <tr>\n      <td>10100</td>\n      <td>0.846000</td>\n    </tr>\n    <tr>\n      <td>10150</td>\n      <td>0.758600</td>\n    </tr>\n    <tr>\n      <td>10200</td>\n      <td>0.825200</td>\n    </tr>\n    <tr>\n      <td>10250</td>\n      <td>0.809300</td>\n    </tr>\n    <tr>\n      <td>10300</td>\n      <td>0.809100</td>\n    </tr>\n    <tr>\n      <td>10350</td>\n      <td>0.828400</td>\n    </tr>\n    <tr>\n      <td>10400</td>\n      <td>0.820100</td>\n    </tr>\n    <tr>\n      <td>10450</td>\n      <td>0.802000</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.758600</td>\n    </tr>\n    <tr>\n      <td>10550</td>\n      <td>0.826300</td>\n    </tr>\n    <tr>\n      <td>10600</td>\n      <td>0.754600</td>\n    </tr>\n    <tr>\n      <td>10650</td>\n      <td>0.749400</td>\n    </tr>\n    <tr>\n      <td>10700</td>\n      <td>0.739900</td>\n    </tr>\n    <tr>\n      <td>10750</td>\n      <td>0.824300</td>\n    </tr>\n    <tr>\n      <td>10800</td>\n      <td>0.713700</td>\n    </tr>\n    <tr>\n      <td>10850</td>\n      <td>0.698500</td>\n    </tr>\n    <tr>\n      <td>10900</td>\n      <td>0.754700</td>\n    </tr>\n    <tr>\n      <td>10950</td>\n      <td>0.756600</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.776800</td>\n    </tr>\n    <tr>\n      <td>11050</td>\n      <td>0.724900</td>\n    </tr>\n    <tr>\n      <td>11100</td>\n      <td>0.758000</td>\n    </tr>\n    <tr>\n      <td>11150</td>\n      <td>0.660900</td>\n    </tr>\n    <tr>\n      <td>11200</td>\n      <td>0.694600</td>\n    </tr>\n    <tr>\n      <td>11250</td>\n      <td>0.746000</td>\n    </tr>\n    <tr>\n      <td>11300</td>\n      <td>0.849100</td>\n    </tr>\n    <tr>\n      <td>11350</td>\n      <td>0.677800</td>\n    </tr>\n    <tr>\n      <td>11400</td>\n      <td>0.678500</td>\n    </tr>\n    <tr>\n      <td>11450</td>\n      <td>0.746600</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.663300</td>\n    </tr>\n    <tr>\n      <td>11550</td>\n      <td>0.667000</td>\n    </tr>\n    <tr>\n      <td>11600</td>\n      <td>0.692300</td>\n    </tr>\n    <tr>\n      <td>11650</td>\n      <td>0.735800</td>\n    </tr>\n    <tr>\n      <td>11700</td>\n      <td>0.692100</td>\n    </tr>\n    <tr>\n      <td>11750</td>\n      <td>0.692300</td>\n    </tr>\n    <tr>\n      <td>11800</td>\n      <td>0.727200</td>\n    </tr>\n    <tr>\n      <td>11850</td>\n      <td>0.683600</td>\n    </tr>\n    <tr>\n      <td>11900</td>\n      <td>0.752500</td>\n    </tr>\n    <tr>\n      <td>11950</td>\n      <td>0.685200</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.700500</td>\n    </tr>\n    <tr>\n      <td>12050</td>\n      <td>0.582100</td>\n    </tr>\n    <tr>\n      <td>12100</td>\n      <td>0.692800</td>\n    </tr>\n    <tr>\n      <td>12150</td>\n      <td>0.692600</td>\n    </tr>\n    <tr>\n      <td>12200</td>\n      <td>0.626900</td>\n    </tr>\n    <tr>\n      <td>12250</td>\n      <td>0.644400</td>\n    </tr>\n    <tr>\n      <td>12300</td>\n      <td>0.631900</td>\n    </tr>\n    <tr>\n      <td>12350</td>\n      <td>0.630700</td>\n    </tr>\n    <tr>\n      <td>12400</td>\n      <td>0.590100</td>\n    </tr>\n    <tr>\n      <td>12450</td>\n      <td>0.597400</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.590500</td>\n    </tr>\n    <tr>\n      <td>12550</td>\n      <td>0.622400</td>\n    </tr>\n    <tr>\n      <td>12600</td>\n      <td>0.656100</td>\n    </tr>\n    <tr>\n      <td>12650</td>\n      <td>0.565900</td>\n    </tr>\n    <tr>\n      <td>12700</td>\n      <td>0.541800</td>\n    </tr>\n    <tr>\n      <td>12750</td>\n      <td>0.651600</td>\n    </tr>\n    <tr>\n      <td>12800</td>\n      <td>0.605300</td>\n    </tr>\n    <tr>\n      <td>12850</td>\n      <td>0.575200</td>\n    </tr>\n    <tr>\n      <td>12900</td>\n      <td>0.666800</td>\n    </tr>\n    <tr>\n      <td>12950</td>\n      <td>0.585800</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.508000</td>\n    </tr>\n    <tr>\n      <td>13050</td>\n      <td>0.724800</td>\n    </tr>\n    <tr>\n      <td>13100</td>\n      <td>0.634700</td>\n    </tr>\n    <tr>\n      <td>13150</td>\n      <td>0.597800</td>\n    </tr>\n    <tr>\n      <td>13200</td>\n      <td>0.718700</td>\n    </tr>\n    <tr>\n      <td>13250</td>\n      <td>0.647000</td>\n    </tr>\n    <tr>\n      <td>13300</td>\n      <td>0.632400</td>\n    </tr>\n    <tr>\n      <td>13350</td>\n      <td>0.641800</td>\n    </tr>\n    <tr>\n      <td>13400</td>\n      <td>0.776900</td>\n    </tr>\n    <tr>\n      <td>13450</td>\n      <td>0.657500</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>0.568400</td>\n    </tr>\n    <tr>\n      <td>13550</td>\n      <td>0.561000</td>\n    </tr>\n    <tr>\n      <td>13600</td>\n      <td>0.592100</td>\n    </tr>\n    <tr>\n      <td>13650</td>\n      <td>0.618700</td>\n    </tr>\n    <tr>\n      <td>13700</td>\n      <td>0.562200</td>\n    </tr>\n    <tr>\n      <td>13750</td>\n      <td>0.579400</td>\n    </tr>\n    <tr>\n      <td>13800</td>\n      <td>0.602900</td>\n    </tr>\n    <tr>\n      <td>13850</td>\n      <td>0.642200</td>\n    </tr>\n    <tr>\n      <td>13900</td>\n      <td>0.613000</td>\n    </tr>\n    <tr>\n      <td>13950</td>\n      <td>0.546800</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>0.572700</td>\n    </tr>\n    <tr>\n      <td>14050</td>\n      <td>0.645600</td>\n    </tr>\n    <tr>\n      <td>14100</td>\n      <td>0.568600</td>\n    </tr>\n    <tr>\n      <td>14150</td>\n      <td>0.673800</td>\n    </tr>\n    <tr>\n      <td>14200</td>\n      <td>0.566300</td>\n    </tr>\n    <tr>\n      <td>14250</td>\n      <td>0.626000</td>\n    </tr>\n    <tr>\n      <td>14300</td>\n      <td>0.617900</td>\n    </tr>\n    <tr>\n      <td>14350</td>\n      <td>0.622500</td>\n    </tr>\n    <tr>\n      <td>14400</td>\n      <td>0.601400</td>\n    </tr>\n    <tr>\n      <td>14450</td>\n      <td>0.524100</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>0.559600</td>\n    </tr>\n    <tr>\n      <td>14550</td>\n      <td>0.564900</td>\n    </tr>\n    <tr>\n      <td>14600</td>\n      <td>0.544700</td>\n    </tr>\n    <tr>\n      <td>14650</td>\n      <td>0.516600</td>\n    </tr>\n    <tr>\n      <td>14700</td>\n      <td>0.603400</td>\n    </tr>\n    <tr>\n      <td>14750</td>\n      <td>0.546300</td>\n    </tr>\n    <tr>\n      <td>14800</td>\n      <td>0.613300</td>\n    </tr>\n    <tr>\n      <td>14850</td>\n      <td>0.544400</td>\n    </tr>\n    <tr>\n      <td>14900</td>\n      <td>0.511900</td>\n    </tr>\n    <tr>\n      <td>14950</td>\n      <td>0.521400</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>0.568500</td>\n    </tr>\n    <tr>\n      <td>15050</td>\n      <td>0.554400</td>\n    </tr>\n    <tr>\n      <td>15100</td>\n      <td>0.566800</td>\n    </tr>\n    <tr>\n      <td>15150</td>\n      <td>0.515000</td>\n    </tr>\n    <tr>\n      <td>15200</td>\n      <td>0.626200</td>\n    </tr>\n    <tr>\n      <td>15250</td>\n      <td>0.508100</td>\n    </tr>\n    <tr>\n      <td>15300</td>\n      <td>0.528600</td>\n    </tr>\n    <tr>\n      <td>15350</td>\n      <td>0.538900</td>\n    </tr>\n    <tr>\n      <td>15400</td>\n      <td>0.542700</td>\n    </tr>\n    <tr>\n      <td>15450</td>\n      <td>0.502200</td>\n    </tr>\n    <tr>\n      <td>15500</td>\n      <td>0.459500</td>\n    </tr>\n    <tr>\n      <td>15550</td>\n      <td>0.557100</td>\n    </tr>\n    <tr>\n      <td>15600</td>\n      <td>0.595300</td>\n    </tr>\n    <tr>\n      <td>15650</td>\n      <td>0.551700</td>\n    </tr>\n    <tr>\n      <td>15700</td>\n      <td>0.509700</td>\n    </tr>\n    <tr>\n      <td>15750</td>\n      <td>0.547900</td>\n    </tr>\n    <tr>\n      <td>15800</td>\n      <td>0.450400</td>\n    </tr>\n    <tr>\n      <td>15850</td>\n      <td>0.525200</td>\n    </tr>\n    <tr>\n      <td>15900</td>\n      <td>0.509900</td>\n    </tr>\n    <tr>\n      <td>15950</td>\n      <td>0.531900</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>0.525500</td>\n    </tr>\n    <tr>\n      <td>16050</td>\n      <td>0.427900</td>\n    </tr>\n    <tr>\n      <td>16100</td>\n      <td>0.484900</td>\n    </tr>\n    <tr>\n      <td>16150</td>\n      <td>0.459900</td>\n    </tr>\n    <tr>\n      <td>16200</td>\n      <td>0.442600</td>\n    </tr>\n    <tr>\n      <td>16250</td>\n      <td>0.420300</td>\n    </tr>\n    <tr>\n      <td>16300</td>\n      <td>0.445800</td>\n    </tr>\n    <tr>\n      <td>16350</td>\n      <td>0.510000</td>\n    </tr>\n    <tr>\n      <td>16400</td>\n      <td>0.513200</td>\n    </tr>\n    <tr>\n      <td>16450</td>\n      <td>0.393700</td>\n    </tr>\n    <tr>\n      <td>16500</td>\n      <td>0.473600</td>\n    </tr>\n    <tr>\n      <td>16550</td>\n      <td>0.416800</td>\n    </tr>\n    <tr>\n      <td>16600</td>\n      <td>0.430600</td>\n    </tr>\n    <tr>\n      <td>16650</td>\n      <td>0.391800</td>\n    </tr>\n    <tr>\n      <td>16700</td>\n      <td>0.458400</td>\n    </tr>\n    <tr>\n      <td>16750</td>\n      <td>0.371400</td>\n    </tr>\n    <tr>\n      <td>16800</td>\n      <td>0.420400</td>\n    </tr>\n    <tr>\n      <td>16850</td>\n      <td>0.419200</td>\n    </tr>\n    <tr>\n      <td>16900</td>\n      <td>0.433200</td>\n    </tr>\n    <tr>\n      <td>16950</td>\n      <td>0.441100</td>\n    </tr>\n    <tr>\n      <td>17000</td>\n      <td>0.473900</td>\n    </tr>\n    <tr>\n      <td>17050</td>\n      <td>0.392600</td>\n    </tr>\n    <tr>\n      <td>17100</td>\n      <td>0.435200</td>\n    </tr>\n    <tr>\n      <td>17150</td>\n      <td>0.366100</td>\n    </tr>\n    <tr>\n      <td>17200</td>\n      <td>0.405700</td>\n    </tr>\n    <tr>\n      <td>17250</td>\n      <td>0.392000</td>\n    </tr>\n    <tr>\n      <td>17300</td>\n      <td>0.433400</td>\n    </tr>\n    <tr>\n      <td>17350</td>\n      <td>0.475600</td>\n    </tr>\n    <tr>\n      <td>17400</td>\n      <td>0.383700</td>\n    </tr>\n    <tr>\n      <td>17450</td>\n      <td>0.473000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"./mon_modele_wav2vec2_arabe\")\nprocessor.save_pretrained(\"./mon_modele_wav2vec2_arabe\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom datasets import load_metric\n\n# 1. Faire les prédictions sur la validation\npredictions_output = trainer.predict(dataset[\"validation\"])\n\n# 2. Convertir logits en ids\npred_ids = torch.argmax(torch.tensor(predictions_output.predictions), dim=-1)\n\n# 3. Décoder les ids en texte\npred_texts = processor.batch_decode(pred_ids)\n\n# 4. Afficher quelques prédictions vs références\nfor i in range(5):\n    print(f\"Prediction: {pred_texts[i]}\")\n    print(f\"Reference : {dataset['validation'][i]['sentence']}\")\n    print(\"---\")\n\n# 5. Calculer le WER\nwer_metric = load_metric(\"wer\")\nreferences = [x[\"sentence\"] for x in dataset[\"validation\"]]\nwer = wer_metric.compute(predictions=pred_texts, references=references)\nprint(f\"WER: {wer:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T15:00:21.026979Z","iopub.execute_input":"2025-05-21T15:00:21.027644Z","iopub.status.idle":"2025-05-21T15:00:21.053719Z","shell.execute_reply.started":"2025-05-21T15:00:21.027616Z","shell.execute_reply":"2025-05-21T15:00:21.052588Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2933442814.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 1. Faire les prédictions sur la validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpredictions_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'load_metric' from 'datasets' (/usr/local/lib/python3.11/dist-packages/datasets/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'load_metric' from 'datasets' (/usr/local/lib/python3.11/dist-packages/datasets/__init__.py)","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"a= 3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T15:00:08.159995Z","iopub.execute_input":"2025-05-21T15:00:08.160674Z","iopub.status.idle":"2025-05-21T15:00:08.164089Z","shell.execute_reply.started":"2025-05-21T15:00:08.160650Z","shell.execute_reply":"2025-05-21T15:00:08.163447Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T15:00:10.649674Z","iopub.execute_input":"2025-05-21T15:00:10.650186Z","iopub.status.idle":"2025-05-21T15:00:10.672515Z","shell.execute_reply.started":"2025-05-21T15:00:10.650167Z","shell.execute_reply":"2025-05-21T15:00:10.671549Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/378453753.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'A' is not defined"],"ename":"NameError","evalue":"name 'A' is not defined","output_type":"error"}],"execution_count":13},{"cell_type":"code","source":"import os\n\ncheckpoint_dir = \"/kaggle/working/wav2vec2-arabic-testA/checkpoint-4000\"  # ou le bon chemin vers ton dossier\nprint(os.listdir(checkpoint_dir))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T15:02:42.453003Z","iopub.execute_input":"2025-05-21T15:02:42.453618Z","iopub.status.idle":"2025-05-21T15:02:42.458015Z","shell.execute_reply.started":"2025-05-21T15:02:42.453595Z","shell.execute_reply":"2025-05-21T15:02:42.457213Z"}},"outputs":[{"name":"stdout","text":"['scaler.pt', 'model.safetensors', 'training_args.bin', 'scheduler.pt', 'config.json', 'rng_state.pth', 'preprocessor_config.json', 'optimizer.pt', 'trainer_state.json']\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict\n\n# Charger directement splits train, validation et test (version publique Common Voice)\ndataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"ar\", split={\n    \"train\": \"train[:4000]\",\n    \"validation\": \"validation[:100]\",\n    \"test\": \"test[:100]\"\n})\n\n# Créer le DatasetDict final (utile si tu veux le manipuler comme un ensemble structuré)\nfinal_dataset = DatasetDict({\n    \"train\": dataset[\"train\"],\n    \"validation\": dataset[\"validation\"],\n    \"test\": dataset[\"test\"]\n})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T13:04:08.308662Z","iopub.execute_input":"2025-05-22T13:04:08.309086Z","iopub.status.idle":"2025-05-22T13:04:09.168061Z","shell.execute_reply.started":"2025-05-22T13:04:08.309043Z","shell.execute_reply":"2025-05-22T13:04:09.167149Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"final_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T12:52:03.959469Z","iopub.status.idle":"2025-05-22T12:52:03.959836Z","shell.execute_reply.started":"2025-05-22T12:52:03.959656Z","shell.execute_reply":"2025-05-22T12:52:03.959672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nfrom datasets import Audio  # <-- IMPORTANT : importer Audio depuis datasets, pas IPython.display\n\n# 4. Charger et caster l'audio correctement\nfinal_dataset = final_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n\n# 5. Normaliser les transcriptions\ndef normalize_text(batch):\n    text = batch[\"sentence\"]\n    text = re.sub(r\"[^\\u0600-\\u06FF\\s]\", \"\", text)  # Garder que les caractères arabes\n    batch[\"sentence\"] = text.strip()\n    return batch\n\nfinal_dataset = final_dataset.map(normalize_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T13:04:12.491987Z","iopub.execute_input":"2025-05-22T13:04:12.492368Z","iopub.status.idle":"2025-05-22T13:04:12.532674Z","shell.execute_reply.started":"2025-05-22T13:04:12.492342Z","shell.execute_reply":"2025-05-22T13:04:12.531767Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from transformers import Wav2Vec2Processor\nfrom datasets import load_dataset, concatenate_datasets, DatasetDict\n\n# Charger le processor arabe\nprocessor = Wav2Vec2Processor.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-arabic\")\n\ndef prepare_dataset(batch):\n    audio = batch[\"audio\"]\n    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=16000).input_values[0]\n    batch[\"labels\"] = processor.tokenizer(batch[\"sentence\"]).input_ids\n    return batch\n\n# Appliquer la fonction de prétraitement\nfinal_dataset = final_dataset.map(prepare_dataset, remove_columns=final_dataset[\"train\"].column_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T13:04:17.512145Z","iopub.execute_input":"2025-05-22T13:04:17.512833Z","iopub.status.idle":"2025-05-22T13:04:18.487924Z","shell.execute_reply.started":"2025-05-22T13:04:17.512807Z","shell.execute_reply":"2025-05-22T13:04:18.487139Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import List, Dict, Union\nimport torch\n\n@dataclass\nclass DataCollatorCTCWithPadding:\n    processor: any\n    padding: Union[bool, str] = True\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n\n        batch = self.processor.pad(input_features, padding=self.padding, return_tensors=\"pt\")\n\n        with self.processor.as_target_processor():\n            labels_batch = self.processor.pad(label_features, padding=self.padding, return_tensors=\"pt\")\n\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n        batch[\"labels\"] = labels\n\n        return batch\n\n# Ensuite :\ndata_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T13:04:23.649824Z","iopub.execute_input":"2025-05-22T13:04:23.650165Z","iopub.status.idle":"2025-05-22T13:04:23.658244Z","shell.execute_reply.started":"2025-05-22T13:04:23.650138Z","shell.execute_reply":"2025-05-22T13:04:23.657357Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import List, Dict, Union\nimport torch\n\n@dataclass\nclass DataCollatorCTCWithPadding:\n    processor: any\n    padding: Union[bool, str] = True\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n\n        batch = self.processor.pad(input_features, padding=self.padding, return_tensors=\"pt\")\n\n        with self.processor.as_target_processor():\n            labels_batch = self.processor.pad(label_features, padding=self.padding, return_tensors=\"pt\")\n\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n        batch[\"labels\"] = labels\n\n        return batch\n\n# Ensuite :\ndata_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T13:00:38.491480Z","iopub.execute_input":"2025-05-22T13:00:38.491716Z","iopub.status.idle":"2025-05-22T13:00:39.409064Z","shell.execute_reply.started":"2025-05-22T13:00:38.491697Z","shell.execute_reply":"2025-05-22T13:00:39.408099Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import evaluate\n\nwer_metric = evaluate.load(\"wer\")\n\ndef compute_metrics(pred):\n    pred_logits = pred.predictions\n    pred_ids = pred_logits.argmax(axis=-1)\n    pred_str = processor.batch_decode(pred_ids)\n    \n    # enlever les -100 dans les labels pour calcul correct\n    label_ids = pred.label_ids\n    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n    label_str = processor.batch_decode(label_ids, group_tokens=False)\n    \n    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n    return {\"wer\": wer}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T13:04:28.558708Z","iopub.execute_input":"2025-05-22T13:04:28.559043Z","iopub.status.idle":"2025-05-22T13:04:29.011895Z","shell.execute_reply.started":"2025-05-22T13:04:28.559020Z","shell.execute_reply":"2025-05-22T13:04:29.010972Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T12:55:00.717532Z","iopub.execute_input":"2025-05-22T12:55:00.717879Z","iopub.status.idle":"2025-05-22T12:55:07.341777Z","shell.execute_reply.started":"2025-05-22T12:55:00.717857Z","shell.execute_reply":"2025-05-22T12:55:07.340797Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nCollecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, evaluate\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.3 fsspec-2025.3.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, TrainingArguments, Trainer\nimport evaluate\nmodel = Wav2Vec2ForCTC.from_pretrained(\n    \"jonatasgrosman/wav2vec2-large-xlsr-53-arabic\",\n    ctc_loss_reduction=\"mean\",\n    pad_token_id=processor.tokenizer.pad_token_id\n)\n\n# 9. Metrics\nwer = evaluate.load(\"wer\")\n\ndef compute_metrics(pred):\n    pred_logits = pred.predictions\n    pred_ids = np.argmax(pred_logits, axis=-1)\n    pred_str = processor.batch_decode(pred_ids)\n    label_ids = pred.label_ids\n    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n    label_str = processor.batch_decode(label_ids, group_tokens=False)\n    return {\"wer\": wer.compute(predictions=pred_str, references=label_str)}\n\n# 10. Arguments d'entraînement\ntraining_args = TrainingArguments(\n    output_dir=\"./wav2vec2-ar\",\n    per_device_train_batch_size=8,  # essaie 16 si la mémoire le permet\n    per_device_eval_batch_size=8,\n    learning_rate=3e-4,\n    num_train_epochs=4,\n    warmup_steps=500,\n    logging_steps=1000,\n    save_steps=500,\n    save_total_limit=2,\n    fp16=True,\n    fp16_full_eval=True,\n    gradient_checkpointing=True,\n    dataloader_num_workers=2,\n    logging_dir=\"./logs\",\n    report_to=\"none\",\n    push_to_hub=False,\n)\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T13:05:38.074413Z","iopub.execute_input":"2025-05-22T13:05:38.074878Z","iopub.status.idle":"2025-05-22T13:05:39.568542Z","shell.execute_reply.started":"2025-05-22T13:05:38.074837Z","shell.execute_reply":"2025-05-22T13:05:39.567669Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=final_dataset[\"train\"],\n    eval_dataset=final_dataset[\"test\"],\n    tokenizer=processor.feature_extractor,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n# Démarrer l'entraînement\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T13:05:42.176742Z","iopub.execute_input":"2025-05-22T13:05:42.177058Z","iopub.status.idle":"2025-05-22T16:30:38.986231Z","shell.execute_reply.started":"2025-05-22T13:05:42.177035Z","shell.execute_reply":"2025-05-22T16:30:38.984968Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_169/3795978614.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 3:24:40, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1000</td>\n      <td>0.379200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1000, training_loss=0.3792135314941406, metrics={'train_runtime': 12295.731, 'train_samples_per_second': 1.301, 'train_steps_per_second': 0.081, 'total_flos': 3.8676255308055245e+18, 'train_loss': 0.3792135314941406, 'epoch': 4.0})"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"import os\n\n# Remplace ce chemin par celui de ton répertoire de sortie\npath = \"/root/.kaggle/competitions/nom-de-la-competition\"  # ou /kaggle/input/nom-du-dataset\n\nfor root, dirs, files in os.walk(path):\n    for file in files:\n        print(os.path.join(root, file))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip /kaggle/working/wav2vec2-ar/checkpoint-1000.zip   /kaggle/working/wav2vec2-ar/checkpoint-1000","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:37:57.155087Z","iopub.execute_input":"2025-05-22T16:37:57.155936Z","iopub.status.idle":"2025-05-22T16:37:57.387403Z","shell.execute_reply.started":"2025-05-22T16:37:57.155902Z","shell.execute_reply":"2025-05-22T16:37:57.386167Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/wav2vec2-ar/checkpoint-1000/ (stored 0%)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"model.save_pretrained(\"./wav2vec2-ar\")\nprocessor.save_pretrained(\"./wav2vec2-ar\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:41:45.582960Z","iopub.execute_input":"2025-05-22T16:41:45.583369Z","iopub.status.idle":"2025-05-22T16:41:47.836585Z","shell.execute_reply.started":"2025-05-22T16:41:45.583335Z","shell.execute_reply":"2025-05-22T16:41:47.835852Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"!zip -r wav2vec2-ar.zip ./wav2vec2-ar","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:42:44.907312Z","iopub.execute_input":"2025-05-22T16:42:44.907657Z","iopub.status.idle":"2025-05-22T16:51:40.784225Z","shell.execute_reply.started":"2025-05-22T16:42:44.907633Z","shell.execute_reply":"2025-05-22T16:51:40.782977Z"}},"outputs":[{"name":"stdout","text":"  adding: wav2vec2-ar/ (stored 0%)\n  adding: wav2vec2-ar/checkpoint-1000.zip (stored 0%)\n  adding: wav2vec2-ar/preprocessor_config.json (deflated 39%)\n  adding: wav2vec2-ar/checkpoint-1000/ (stored 0%)\n  adding: wav2vec2-ar/checkpoint-1000/training_args.bin (deflated 52%)\n  adding: wav2vec2-ar/checkpoint-1000/trainer_state.json (deflated 56%)\n  adding: wav2vec2-ar/checkpoint-1000/preprocessor_config.json (deflated 35%)\n  adding: wav2vec2-ar/checkpoint-1000/model.safetensors (deflated 7%)\n  adding: wav2vec2-ar/checkpoint-1000/config.json (deflated 67%)\n  adding: wav2vec2-ar/checkpoint-1000/optimizer.pt (deflated 9%)\n  adding: wav2vec2-ar/checkpoint-1000/scaler.pt (deflated 60%)\n  adding: wav2vec2-ar/checkpoint-1000/scheduler.pt (deflated 56%)\n  adding: wav2vec2-ar/checkpoint-1000/rng_state.pth (deflated 25%)\n  adding: wav2vec2-ar/model.safetensors (deflated 7%)\n  adding: wav2vec2-ar/config.json (deflated 67%)\n  adding: wav2vec2-ar/tokenizer_config.json (deflated 71%)\n  adding: wav2vec2-ar/checkpoint-500/ (stored 0%)\n  adding: wav2vec2-ar/checkpoint-500/training_args.bin (deflated 52%)\n  adding: wav2vec2-ar/checkpoint-500/trainer_state.json (deflated 56%)\n  adding: wav2vec2-ar/checkpoint-500/preprocessor_config.json (deflated 35%)\n  adding: wav2vec2-ar/checkpoint-500/model.safetensors (deflated 7%)\n  adding: wav2vec2-ar/checkpoint-500/config.json (deflated 67%)\n  adding: wav2vec2-ar/checkpoint-500/optimizer.pt (deflated 8%)\n  adding: wav2vec2-ar/checkpoint-500/scaler.pt (deflated 60%)\n  adding: wav2vec2-ar/checkpoint-500/scheduler.pt (deflated 57%)\n  adding: wav2vec2-ar/checkpoint-500/rng_state.pth (deflated 25%)\n  adding: wav2vec2-ar/special_tokens_map.json (deflated 45%)\n  adding: wav2vec2-ar/vocab.json (deflated 61%)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import torch\nimport librosa\nimport numpy as np\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\nprocessor = Wav2Vec2Processor.from_pretrained(\"/kaggle/input/process/wav2vec2-darija-processor\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"/kaggle/working/wav2vec2-ar/checkpoint-1000\")\n\ndef transcribe_audio(audio_path):\n    # Charger et resampler audio à 16 kHz\n    speech, sample_rate = librosa.load(audio_path, sr=16000)\n    print(f\"Sample rate après resampling: {sample_rate}, samples: {len(speech)}, duration (s): {len(speech)/sample_rate:.2f}\")\n    \n    # Padding si audio trop court (ici 1 sec = 16000 samples)\n    min_length = 16000\n    if len(speech) < min_length:\n        pad_length = min_length - len(speech)\n        speech = np.pad(speech, (0, pad_length), 'constant', constant_values=0)\n        print(f\"Audio paded to {len(speech)} samples.\")\n\n    input_values = processor(speech, sampling_rate=sample_rate, return_tensors=\"pt\").input_values\n\n    with torch.no_grad():\n        logits = model(input_values).logits\n\n    predicted_ids = torch.argmax(logits, dim=-1)\n    transcription = processor.decode(predicted_ids[0])\n    return transcription\n\naudio_file = \"/kaggle/input/test-arabe/audio (2).mp3\"\ntranscription = transcribe_audio(audio_file)\nprint(f\"Transcription: {transcription}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}